{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Benchmark for Biomedical Knowledge Graph based Similarity \n",
    "\n",
    "This notebook will guide you through the evaluation of semantic similarity measures. [GitHub](https://github.com/liseda-lab/kgsim-benchmark/tree/master) explains how to use the benchmark data sets for calculation and evaluation of semantic similarity measures.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Importing libraries](#importing)\n",
    "2. [Selecting datasets](#datasets)\n",
    "3. [Correlation calculation](#correlation)\n",
    "4. [Correlation plotting](#plot-correlation)\n",
    "5. [Protein-protein interaction prediction](#ppi-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing libraries  <a class=\"anchor\" id=\"importing\"></a>\n",
    "**In this notebook, [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) as well as the Python libraries Pandas, NumPy, Scikit-learn, Matplotlib and SciPy are used.**\n",
    "\n",
    "You should have [Anaconda](https://www.anaconda.com) installed in your computer, which already installs Python 3, Jupyter, Scikit-learn, Pandas, NumPy, Matplotlib and SciPy. \n",
    "\n",
    "If you have any issues, the library versions used in the development of this notebook can be found in the table below.\n",
    "\n",
    "| Library     | Version  |\n",
    "|:-----------:|:--------:|\n",
    "| Anaconda    | 5.3.0    |\n",
    "| Matplotlib  | 3.0.3    |\n",
    "| Numpy       | 1.15.4   | \n",
    "| Pandas      | 0.23.4   | \n",
    "| Scikit-learn| 0.20.1   |\n",
    "| Scipy       | 1.1.0    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ppi_prediction as ppi\n",
    "import evaluating as evlt\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting datasets  <a class=\"anchor\" id=\"datasets\"></a>\n",
    "\n",
    "In this section, you should state the type of benchmark data sets you're dealing with (`dataset_type` = PPI, MF or GENE, case-insensitive) as well as the path for your `test_dataset` and `benchmark_dataset`.\n",
    "\n",
    "The datasets for `benchmark_dataset` can be retrieved from [GitHub](https://github.com/liseda-lab/kgsim-benchmark/tree/master/Data%20Sets).\n",
    "\n",
    "**Beware** that using benchmark and test datasets of different sizes will raise an error in the evaluation and that selecting the wrong dataset_type can lead to wrongly calculated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'PPI, MF or GENE'\n",
    "test_dataset = 'path_to_test_measure.csv'\n",
    "benchmark_dataset = 'path_to_benchmark_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_dataset` should be the path for the dataset containing the test semantic similarity measures. `extract_new_ssm` extracts only the column containing said measure (default is `col_n` = -1). The default delimiter for these datasets is `;`, however it can be changed by adding the parameter `sep`. Additionally, if the dataset has no header, the parameter `head` = `None` should be added. An example of how to use these parameters is in a comment in the cell below.\n",
    "\n",
    "\n",
    "`measures` is the name of the dictionary containing all the measures you want to evaluate, the key being the measure's name (that will feature in the plots and dataframes throughout this notebook) and the value being the calculated similarities for the pairs in the data set using that measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If your target semantic similarity measure is in the first column of a headerless data set, use:\n",
    "#'measure name':evlt.extract_new_ssm(test_dataset, col_n = 0, head = None)\n",
    "\n",
    "measures = {'measure_name':evlt.extract_new_ssm(test_dataset, sep = '', col_n = int)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation calculation  <a class=\"anchor\" id=\"correlation\"></a>\n",
    "This module will calculate Pearson Correlation Coefficient between the test semantic similarity measures and the similarity proxies available for the type of dataset you're working with. This produces a dataset that enables the comparison between state of the art semantic similarity measures and the test semantic similarity measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = evlt.correlation_calculation(benchmark_dataset, measures, dataset_type)\n",
    "display.display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation plotting  <a class=\"anchor\" id=\"plot-correlation\"></a>\n",
    "In this section you can plot each test semantic similarity measure and the similarity proxies available for the type of dataset you're working with. Semantic similarity measures can be plotted against the following proxies, according to dataset type:\n",
    "\n",
    "| Dataset type| Similarity proxy  |\n",
    "|:-----------:|-------------------|\n",
    "| PPI         | sim(seq)|\n",
    "| MF          | sim(seq) and sim(MF) |\n",
    "| GENE        | sim(PS)   | \n",
    "\n",
    "You can costumize your plots by changing the labels for the axis (`xlabel` and `ylabel`) and by adding `size` = `int` in any of the plotting functions. This argument will change the size of the plot points. The color of the plot points can also be changed with the argument `col`. Check [Matplotlib](https://matplotlib.org/3.1.0/gallery/color/named_colors.html) for more information on colors and the comments in the cell below for information on how to use the alternative parameters.\n",
    "\n",
    "If you're working with a MF dataset, two side by side plots will be produced. Alternativley, you can run the two cells below for separate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_measure = \"measure_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evlt.molecular_function_dataset(dataset_type):\n",
    "    plot = evlt.plot_molecular_function(benchmark_dataset, measures[selected_measure])\n",
    "    plot[0].set(xlabel = 'Sequence Similarity', ylabel=selected_measure)\n",
    "    plot[1].set(xlabel = 'Molecular Function Similarity', ylabel=selected_measure)\n",
    "else:\n",
    "    plot = evlt.correlation_plot(benchmark_dataset, measures[selected_measure], dataset_type, selected_measure) \n",
    "    #plt.ylabel('your new ylabel here')\n",
    "    #plt.xlabel('your new xlabel here')\n",
    "    \n",
    "#if you want to costumize any of your plots, e.g. with large pink dots, you should add these parameters to the plotting functions:\n",
    "#plot = evlt.plot_molecular_function(benchmark_dataset, measures[selected_measure], size = 5, col = 'hotpink')\n",
    "#plot = evlt.correlation_plot(benchmark_dataset, measures[selected_measure], dataset_type, size = 5, col = 'hotpink') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evlt.molecular_function_dataset(dataset_type):\n",
    "    plot_pfam = evlt.plotting_molecular_function(benchmark_dataset, measures[selected_measure], selected_measure)\n",
    "    #plt.ylabel('your new ylabel here')\n",
    "    #plt.xlabel('your new xlabel here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evlt.molecular_function_dataset(dataset_type):    \n",
    "    plot_seq = evlt.plotting_sequence(benchmark_dataset, measures[selected_measure], selected_measure)\n",
    "    #plt.ylabel('your new ylabel here')\n",
    "    #plt.xlabel('your new xlabel here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Protein-protein interaction  <a class=\"anchor\" id=\"ppi-prediction\"></a>\n",
    "In case you're dealing with a PPI dataset, it's possible to calculate how well the semantic similarity measures can predict Protein Protein Interactions. This module has two different functionalities:\n",
    "1. Plot a Precision-Recall plot and find the highest F1-score. This is found by using 10-fold cross validation to test a range of different thresholds, selecting the best one, for each measure (state of the art and test).  \n",
    "2. Calculate precision, recall and F1-score for a given threshold (by changing the value of `thresh`). A dataframe and plot comparing precision, recall and F1-score for all semantic similarity measures at that threshold will be produced. The plot points color can be changed by adding the parameter `col`= [list of strings with size = number of measures], where each string is a valid color. You can also add `size` = `int` if you want to change the size of the plot points. \n",
    "\n",
    "For both plots, you can check [Matplotlib](https://matplotlib.org/3.1.0/gallery/color/named_colors.html) for more information on the colors for your plot, or check the comments for code examples. **Beware** that for both plots, the default color list only supports 10 different similarity measures. If you want to plot more than 10 measures in the same plot, you must input your own `col` list with Matplotlib colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ppi.interaction_dataset(dataset_type):\n",
    "        results = ppi.plot_precision_recall(benchmark_dataset, measures)\n",
    "        display.display(results)\n",
    "    #if you don't want to calculate the best threshold for the benchmark measures add the parameter benchmark_measures = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input your threshold of choice\n",
    "thresh = 0.2 \n",
    "\n",
    "if ppi.interaction_dataset(dataset_type):\n",
    "        df_f1 = ppi.get_f1_score_df(benchmark_dataset, measures, thresh)\n",
    "        ppi.plot_f1(df_f1)\n",
    "        display.display(df_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "\n",
    "* **Carlota Cardoso** \n",
    "* **Rita Sousa**\n",
    "* **CÃ¡tia Pesquita** \n",
    "\n",
    "#### License\n",
    "See the [LICENSE.md](https://github.com/liseda-lab/kgsim-benchmark/blob/master/LICENSE.md/LICENSE.md) file for details.\n",
    "\n",
    "#### Acknowledgments\n",
    "\n",
    "This project was funded by the Portuguese FCT through the LASIGE Research Unit (UID/CEC/00408/2019), and also by the SMILAX project (PTDC/EEI-ESS/4633/2014)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
